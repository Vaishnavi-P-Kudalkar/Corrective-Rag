# -*- coding: utf-8 -*-
"""localdocumentcorrectiveragtypeofhal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttNEqVwqFtwmNoG-SYJ78hiz7SszFAK6
"""



from tqdm.notebook import tqdm
import pandas as pd
from typing import Optional, List, Tuple
from datasets import Dataset
import matplotlib.pyplot as plt

pd.set_option("display.max_colwidth", None)  # this will be helpful when visualizing retriever outputs


# Install required packages


# Then import
import fitz  # PyMuPDF
from tqdm import tqdm
from langchain.docstore.document import Document as LangchainDocument

import os
import fitz  # PyMuPDF
from tqdm import tqdm
from langchain.docstore.document import Document as LangchainDocument

# Mount Google Drive (if using Colab)


# Define the folder where your PDFs are stored
DOCUMENTS_FOLDER = "D:/Capstone/corrective_rag/Rag_Documents"  # Update this path

# Load all PDF files from the folder
RAW_KNOWLEDGE_BASE = []

for filename in tqdm(os.listdir(DOCUMENTS_FOLDER)):
    file_path = os.path.join(DOCUMENTS_FOLDER, filename)

    if filename.endswith(".pdf"):  # Process only PDF files
        try:
            with fitz.open(file_path) as doc:
                text = "\n".join([page.get_text("text") for page in doc])  # Extract text from all pages
                RAW_KNOWLEDGE_BASE.append(LangchainDocument(page_content=text, metadata={"source": filename}))
        except Exception as e:
            print(f"Error processing {filename}: {e}")

# Print information about the loaded documents
print(f"Total Documents Loaded: {len(RAW_KNOWLEDGE_BASE)}")
if RAW_KNOWLEDGE_BASE:
    print(RAW_KNOWLEDGE_BASE[0].page_content[:500])  # Print first 500 chars of first document
    print(RAW_KNOWLEDGE_BASE[0].metadata)

from langchain.text_splitter import RecursiveCharacterTextSplitter

# We use a hierarchical list of separators specifically tailored for splitting Markdown documents
MARKDOWN_SEPARATORS = [
    "\n#{1,6} ",
    "```\n",
    "\n\\*\\*\\*+\n",
    "\n---+\n",
    "\n___+\n",
    "\n\n",
    "\n",
    " ",
    "",
]

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # the maximum number of characters in a chunk: we selected this value arbitrarily
    chunk_overlap=100,  # the number of characters to overlap between chunks
    add_start_index=True,  # If `True`, includes chunk's start index in metadata
    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document
    separators=MARKDOWN_SEPARATORS,
)

docs_processed = []
for doc in RAW_KNOWLEDGE_BASE:
    docs_processed += text_splitter.split_documents([doc])

from sentence_transformers import SentenceTransformer

# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter.
print(f"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("thenlper/gte-small")
lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]

# Plot the distribution of document lengths, counted as the number of tokens
fig = pd.Series(lengths).hist()
plt.title("Distribution of document lengths in the knowledge base (in count of tokens)")
plt.show()

from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer

EMBEDDING_MODEL_NAME = "thenlper/gte-small"
chunk_size = 128

text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME),
    chunk_size=chunk_size,
    chunk_overlap=int(chunk_size / 10),
    add_start_index=True,
    strip_whitespace=True,
    separators=MARKDOWN_SEPARATORS,
)

docs_processed = []
for doc in RAW_KNOWLEDGE_BASE:
    docs_processed += text_splitter.split_documents([doc])

print(len(docs_processed))  # 19983

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]
fig = pd.Series(lengths).hist()
plt.title("Distribution of document lengths in the knowledge base (in count of tokens)")
plt.show()




# Ensure you have these imports
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
import os

# Create a specific directory for your vector database
PERSIST_DIRECTORY = "/content/drive/MyDrive/RAG_Vector_Database"
os.makedirs(PERSIST_DIRECTORY, exist_ok=True)

# Create embedding model
embedding_model = HuggingFaceEmbeddings(
    model_name="thenlper/gte-small",  # or your preferred embedding model
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True}
)

# Create vector database
KNOWLEDGE_VECTOR_DATABASE = Chroma.from_documents(
    docs_processed,  # your processed documents
    embedding_model,
    persist_directory=PERSIST_DIRECTORY,
    collection_name="my_document_collection"
)

# Persist the database
KNOWLEDGE_VECTOR_DATABASE.persist()

# Verify documents
print(f"Number of documents in vector database: {len(KNOWLEDGE_VECTOR_DATABASE.get()['documents'])}")

# Retrieve documents, embeddings, and metadata
database_contents = KNOWLEDGE_VECTOR_DATABASE.get(include=['embeddings', 'documents', 'metadatas'])

# Print total number of documents
print(f"Total documents: {len(database_contents['documents'])}")

# Optionally, print first few documents
for i, doc in enumerate(database_contents['documents'][:5]):
    print(f"Document {i}:")
    print(doc)
    print("---")

# If you want to see metadata
for i, metadata in enumerate(database_contents['metadatas'][:5]):
    print(f"Metadata {i}:")
    print(metadata)
    print("---")

user_query = "what are the 2 categories of hallucination?"
print(f"\nStarting retrieval for {user_query=}...")
retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)

user_query = "what are the 2 categories of hallucination?"
print(f"\nStarting retrieval for {user_query=}...")
retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)
print("\n==================================Top document==================================")
print(retrieved_docs[1].page_content)
print("==================================Metadata==================================")
print(retrieved_docs[1].metadata)


from transformers import pipeline
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline

from transformers import AutoModelForCausalLM, AutoTokenizer

READER_MODEL_NAME = "meta-llama/Llama-3.2-1B"  # Make sure this is the correct repo ID

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)

# Load model (make sure you have enough VRAM if using GPU)
model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME)

import torch

device = torch.device("cuda")  # Force CPU mode
embedding_model = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={"device": device}  # Pass the device as a parameter
)


bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)
model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)

READER_LLM = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    do_sample=True,
    temperature=0.2,
    repetition_penalty=1.1,
    return_full_text=False,
    max_new_tokens=500,
)

from langchain.prompts import PromptTemplate

# Create the HuggingFacePipeline LLM
llm = HuggingFacePipeline(pipeline=READER_LLM)

# Define a properly formatted prompt template for Llama 3.2-1B
llama_prompt_template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
Using the information contained in the context,
give a comprehensive answer to the question.
Respond only to the question asked, response should be concise and relevant to the question.
Provide the number of the source document when relevant.
If the answer cannot be deduced from the context, do not give an answer.<|eot_id|>

<|start_header_id|>user<|end_header_id|>
Context:
{context}
Question: {question}<|eot_id|>

<|start_header_id|>assistant<|end_header_id|>
"""

# Create a PromptTemplate
RAG_PROMPT = PromptTemplate(
    template=llama_prompt_template,
    input_variables=["context", "question"]
)


# Define the evaluation metrics functions
from sklearn.metrics import f1_score as sklearn_f1_score
import numpy as np

def f1_score(expected: str, response: str) -> float:
    """
    Calculate F1 score between expected and response.
    Tokenize the expected and response strings and compute precision, recall, and F1 score.
    """
    expected_tokens = set(expected.split())
    response_tokens = set(response.split())

    # Calculate precision and recall
    if len(response_tokens) == 0:
        return 0.0  # Avoid division by zero

    true_positives = len(expected_tokens & response_tokens)
    precision = true_positives / len(response_tokens)
    recall = true_positives / len(expected_tokens)

    if precision + recall == 0:
        return 0.0  # Avoid division by zero

    f1 = 2 * (precision * recall) / (precision + recall)
    return f1

def exact_match(expected: str, response: str) -> int:
    """
    Calculate exact match score between expected and response.
    Returns 1 if exact match, 0 otherwise.
    """
    return int(expected.strip().lower() == response.strip().lower())

from transformers import pipeline, AutoTokenizer
import numpy as np

# Initialize the hallucination evaluation classifier
classifier = pipeline(
    "text-classification",
    model='vectara/hallucination_evaluation_model',
    tokenizer=AutoTokenizer.from_pretrained('google/flan-t5-base'),
    trust_remote_code=True
)

def evaluate_response(response: str, expected: str) -> dict:
    """
    Evaluate response using multiple metrics
    """
    consistency_score = classifier(
        f"<pad> Determine if the following response aligns with factual truth:\n\nResponse: {response}\n\nExpected: {expected}"
    )[0]['score']
    em_score = exact_match(expected, response)
    f1_score_val = f1_score(expected, response)  # Calculate F1 score
    return {
        'consistency_score': consistency_score,
        'exact_match': em_score,
        'f1_score': f1_score_val
    }

from ragatouille import RAGPretrainedModel

RERANKER = RAGPretrainedModel.from_pretrained("colbert-ir/colbertv2.0")

# Evaluation function for the RAG system
def evaluate_rag_system(test_cases: List[dict]) -> List[dict]:
    """
    Evaluate RAG system using multiple test cases
    """
    results = []

    for test_case in test_cases:
        # Get retrieved documents
        retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=test_case["question"], k=5)

        # Apply reranking if using ColBERT
        reranked_docs = [doc.page_content for doc in retrieved_docs]
        if RERANKER:
            reranked_results = RERANKER.rerank(test_case["question"], reranked_docs, k=3)
            reranked_docs = [doc["content"] for doc in reranked_results]

        # Create context
        context = "\nExtracted documents:\n"
        context += "".join([f"Document {str(i)}:::\n" + doc for i, doc in enumerate(reranked_docs)])

        # Generate final prompt
        final_prompt = RAG_PROMPT.format(
            question=test_case["question"],
            context=context
        )



        # Get LLM response
        llm_response = llm(final_prompt)

        # Evaluate the response
        evaluation_results = evaluate_response(llm_response, test_case["expected"])

        # Store results
        results.append({
            "question": test_case["question"],
            "response": llm_response,
            "expected": test_case["expected"],
            **evaluation_results
        })

    return results

# Example usage:
test_cases = [
    {
        "question": "brief about the Taj Mahal?",
        "expected": "The Taj Mahal is a white marble mausoleum built by Mughal Emperor Shah Jahan in memory of his wife Mumtaz Mahal."
    },
    {
        "question": "give abstract about the paper Towards understanding and mitigating the hallucinations in NLP and Speech?",
        "expected": "The paper discusses techniques to understand and reduce hallucinations in language and speech models."
    }
]

# Run evaluation
evaluation_results = evaluate_rag_system(test_cases)

# Print results
for i, result in enumerate(evaluation_results, 1):
    print(f"\nTest Case {i}:")
    print(f"Question: {result['question']}")
    print(f"Response: {result['response']}")
    print(f"Expected: {result['expected']}")
    print(f"Consistency Score: {result['consistency_score']:.3f}")
    print(f"Exact Match: {result['exact_match']}")
    print(f"F1 Score: {result['f1_score']:.3f}")

# Calculate and print averages
avg_consistency = np.mean([r['consistency_score'] for r in evaluation_results])
avg_em = np.mean([r['exact_match'] for r in evaluation_results])
avg_f1 = np.mean([r['f1_score'] for r in evaluation_results])

print("\nOverall Metrics:")
print(f"Average Consistency Score: {avg_consistency:.3f}")
print(f"Average Exact Match: {avg_em:.3f}")
print(f"Average F1 Score: {avg_f1:.3f}")
